%
% File eacl2017.tex
%
%% Based on the style files for ACL-2016
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{eacl2017}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}

\eaclfinalcopy % Uncomment this line for the final submission
%\def\eaclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{OpenNMT: Open-Source Toolkit for Neural Machine Translation}

\author{Yoon Kim, Guillaume Klein, Jean Senellart, Alexander M. Rush ... }

\date{}

\begin{document}
\maketitle
\begin{abstract}

  We describe an open-source toolkit for neural machine translation
  that supports research development of sequence-to-sequence models.
  The system is prioritizes simplicity, modularity, and efficiency to
  make it reasonable for researchers to experiment with variants of
  neural machine translation that explore different feature
  representations, model architectures, and source (multi)-modalities,
  while maintaining competitive performance and tractable training
  requirements. The toolkit consists of modeling and decoding support,
  as well as detailed pedagogical documentation about the underlying
  methodologies.

\end{abstract}

\section{Introduction}


\begin{itemize}
\item Description of current state of NMT
\item Difference with other NMT
\item Description of open-source nmt systems. Several research systems, but there is a not a ... 
\end{itemize}


\paragraph{Motivation}

why build a new system.

Some proprietary (google, etc.), others too research-y

Prioritized factors

\begin{itemize}
\item Training efficiency
\item System Modularity
\item Support for alternative representations
\item Industrial decoding.
\end{itemize}


\section{Background: Neural Machine Translation}

\begin{itemize}
\item One column describing the technical details
\end{itemize}

\section{Implementation}

Details: torch. text-to-text mapping. toolkit for extensibility

\subsection{Modularity}

\begin{itemize}
\item Separate encoder/decoder 
\item Arbitrary input/output representations (features, images)
\end{itemize}

\subsection{Optimizations}

Encoder/Decoder optimization

\begin{itemize}
\item Shared memory (cite opt net)
\item C-Decoder  
\end{itemize}

\subsection{Advanced Features}

Other papers . 

\section{User Studies}

\paragraph{Feature-Based Inputs}

Discuss using modularity to add features as input and output

\paragraph{Knowledge Distillation/Pruning}

Discuss model access to allow for pruning and distillation. 

\paragraph{Im2Latex}

Discuss ability to modify encoder representation to allow for other tasks. 

\section{Experimental Results}

Mainly focus on NMT. Speed, memory, accuracy. 

\begin{table}
  \centering
  
  \caption{Performance Results. Several languages}
\end{table}


\begin{table}
  \centering
  
  \caption{Speed Results. Multi-GPU, distillation, c decoder}
\end{table}

Picture of demo application running 

\section{Conclusion}


\bibliography{writeup}
\bibliographystyle{eacl2017}


\end{document}
